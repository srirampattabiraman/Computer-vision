{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bad network for MNIST dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srirampattabiraman/Computer-vision/blob/master/Bad_network_for_MNIST_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nJ7YEw_vyjG",
        "colab_type": "text"
      },
      "source": [
        "# **Not an ideal network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu6gs3x2R0R3",
        "colab_type": "text"
      },
      "source": [
        "The cell below does the following:\n",
        "\n",
        "- It issues a **shell command** to install the keras module into the python environment that this notebook is supposed to run in\n",
        "- It imports the keras module into the program written in this notebook\n",
        "\n",
        "The **!** symbol is used in notebooks to issue **shell commands**. The **shell** is an typed interface to a computer. One could use shells like the *bash* (UNIX based systems) or *powershell* (Windows based systems) to execute binaries, move files and folders and what have you.\n",
        "\n",
        "A **python module** is synonymous to a library in other programming language. It is the grouping of multiple classes and methods under a namespace. When imported to another program, these methods and classes can be used in the programusing the syntax:\n",
        "    \n",
        "    <module_name>.<class/method_name>\n",
        "    \n",
        "A **python environment** is simply a fancy name for the runtime. It includes the python interpreter, the site-packages, a set of binaries like pip, pip3 etc and ofcourse the packages that were manually installed. Environments are used to maintain repeatability of behaviour of code and facilitate collaboration among programmers.\n",
        "\n",
        "**Keras** is an open source wrapper API using Google's tensorflow framework. It helps to think of *keras* as an easy way to build neural networks and perform mathematical computation using the tensorflow backend.\n",
        "\n",
        "**Tensorflow** is an opensource mathematical computation framework that operates on predefined computational graphs. It was developed and is maintained by Google and is one of the most hit repositories on GitHub.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGWv5hBhv2jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57YJWKwkSZYk",
        "colab_type": "text"
      },
      "source": [
        "The block of code below imports classes and methods and whole python modules into the runtime. We shall go through each one of the imports and see why we have imported them:\n",
        "\n",
        "- **numpy** : NumPy is a python library used for mathematical computation. Machine learning frameworks like Tensorflow, Pythorch etc. interface very well with numpy. Inout is served to the ML frameworks using NumPy arrays and the frameworks variables and constants can be converted to numpy N-D arrays. Hence it is a very handy tool to have\n",
        "\n",
        "- **Sequential** : Sequential is a class defined in keras. It is a blueprint for a model consisting of layers stacked on top of each other in a *linear order*. It does not permit branching from any of it's layers. ([Link to documentation](https://keras.io/getting-started/sequential-model-guide/)) \n",
        "\n",
        "- **Flatten** : Flatten is a class defined in keras that implements a Layer ADT (Abstract Datatype). A Layer is a class that defines a set of functions - forward and backward for example. Flatten is a Layer whose forward pass takes an input of size *(batch_size, num_channels, height, width)* and flattens the last three dimentions resulting in a tensor of shape *(batch_size, num_channels * height * width)*. Well to be fair it only creates a tensorflow Op that actuates the flatten operation([Link to documentation](https://keras.io/layers/core/))\n",
        "\n",
        "- **Convolution2D** - This is again a Layer which expects as input an Image of shape *(batch_size, num_channels, height, width)* and kernel parameters in the order *num_filters, height, width*. It creates a tensorflow operation that convolves the filter on the Image and outputs the result in the shape *batch_size, height, width* - You can also pass in the weight initialization information, a switch for adding the bias, regularization parameters etc. ([Link to documentation](http://faroit.com/keras-docs/1.2.2/layers/convolutional/))\n",
        "\n",
        "- **np_utils** : These provide a ton on awesome and generally used methods for building the data-pipeline. The one we are interested in today is the *to_categorical* method that takes as input an inout array of shape *(batch_size)* where each value belongs to the set N[0, num_classes - 1] where N stands for the set of Natural numbers and converts it to a one - hot format - a tensor of shape *(batch_size, num_classes)*. The *one hot* format encodes the class label*(L)* as a pulse (A zero vector *v* of shape *(num_classes)* where only v[L] = 1.0)\n",
        "\n",
        "- **mnist** : It is a dataset comprising of 60,000 *(1, 28, 28)* shape greyscale images of handwritten digits. The static *load_data*  method of mnist returns 2 numpy arrays of shape *(num_samples, 28, 28)* representing the train and the test sets and 2 numpy arrays of shape *(num_samples)* representing the train and test labels. The trainset comprises of 50,000 examples and the testset comprises of 10,000 examples. ([Link to documentation](https://keras.io/datasets/))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnMlDJQKv4VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Convolution2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBOR1YMiyJO_",
        "colab_type": "text"
      },
      "source": [
        "The code below loads the images and labels belonging to the mnist dataset into numpy arrays. Well, it's actually not all that simple. To appreciate what the *mnist.load_data()* method has abstracted away - let's dig a little deeper:\n",
        "\n",
        "- **Download the data** : For one, the actual files containing the images and the labels are not in the system where this program is running.  The method actually takes an argument called *PATH* that is by default an empty string. It looks up the system at <~/keras/datasets/ + PATH> for the image and label files and if not found it downloads them from the internet and stores them at that location. All non-existent directories are created on the go.\n",
        "\n",
        "- **Convert images to arrays** : Well what gets downloaded is a *.npz* file - this is numpys way of storing arrays encountered during the runtime as tar.gz files. NumPy provides I/O utilities for these kind of files. Hence, after downloading the .npz files the method loads them into the runtime as numpy arrays\n",
        "\n",
        "- **Returns the arrays** : The arrays thus loaded of shapes X_train(60000, 28, 28), y_train(60000), X_test(10000, 28, 28) and y_test(10000) are packed in tuples as ((X_train, y_train),(X_test,y_test)) and returned from the method "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CdSu2lMwB9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEPjIPfQqV1t",
        "colab_type": "text"
      },
      "source": [
        "The code below does the following:\n",
        "\n",
        "- **Printing the shape of the training images** : X_train.shape prints out the shape of the numpy ND array X_train. A shape is a property of a numpy ND array object that packs the lengths of each of it's dimentions into a tuple. In this case, the shape is *(60000, 28, 28)* standing for *(batch_size, height, width)*\n",
        "\n",
        "- **Imports matplotlib.pyplot** : This is a plotting module used with python to plot graphs, summary plots, images and what have you.\n",
        "\n",
        "- **Plots the first example of the training set** : Here the code uses *matplotlib.pyplot.imshow()* method to plot the 0th element of X_train which is the first training image.\n",
        "\n",
        "*PS : %matplotlin inline is a rendering directive to matplotlib to use the console to print the plot and not a separate window*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLaDf0-rwCmj",
        "colab_type": "code",
        "outputId": "f11a3409-d7c8-4c2b-d175-a4466dc81811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fca58ab1710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl25eHG4AKMI",
        "colab_type": "text"
      },
      "source": [
        "The code below simply reshapes the train and test sets of images - variables named *X_train* and *X_test* respectively. Why?\n",
        "\n",
        "Well it so happens that under the hood the *Convolution2D* layer that we imported earlier calls the tensorflow.nn.conv2D() method to create a convolution operation node in the computational graph on the network.\n",
        "\n",
        "[This operation](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) takes an argument called data_format which defaults to *'NHCW'* - meaning that the inoput is expected in the shape *[batch_size, height, width, num_channels]*\n",
        "\n",
        "Since mnist Images are greyscale - they contain only one color channel which we make explicit when reshaping X_train and X_test to *(train/test_set_size, 28, 28, 1)*\n",
        "\n",
        "- X_train[0] --> train_set_size\n",
        "- X-test[0] --> test_set_size "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erb11jNwwFwl",
        "colab_type": "code",
        "outputId": "fd7a0a88-2306-4876-b2d7-adf48f197e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)\n",
        "print(type(X_train[0,0,0,0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.uint8'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xRXDx7MDA2d",
        "colab_type": "text"
      },
      "source": [
        "The code below performs the following actions:\n",
        "\n",
        "###Type Conversion\n",
        "\n",
        "X_train and X_test are currently of type *numpy.uint8* which is a numpy wrapper over the unsigned 4-byte integer python datatype. We convert them to *numpy.float32* for the following reasons\n",
        "\n",
        "- The *tensorflow.nn.conv2d* operation expects the input tensor to be of *tf.half*, *tf.bfloat*, *tf.float32* or *tf.float64* datatypes and numpy arrays of datatype *uint32* arent convertible to any of these datatypes. Hence, we convert the input to type *np.float32* in the code below.\n",
        "\n",
        "- We need to scale the pixels of the image within the range R[0,1] where R stands for the set of real numbers and integer division just wont work.\n",
        "\n",
        "###Input Scaling\n",
        "\n",
        "Okay then .. why scale at all? Well think of it this way the *SGD* algorithm used to train neural networks uses the gradient of the loss w.r.t the kernels to train / update them for the next batch. Therefore, the delta in the kernel values would be proportional to the gradients which in turn would be a function of the input values.\n",
        "\n",
        "Alright - so if we permit a range between R[0.0, 255.0], we are also permitting a wide range of gradients - Hence the correction for not detecting a llight edge would be significantly smaller than the correction for not detecting a bright edge and this is the behaviour we want to eliminate from our networks. Our judgement of an error should be agnostic to the intensity of the input - and hence we scale down to R[0,1]\n",
        "\n",
        "Well, we've added code to ensure that the scaling has not messed up with out features - Plotting the image, we see the more or less look the same before and after scaling.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLK4YDoRwHet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzts-rDKMc1t",
        "colab_type": "text"
      },
      "source": [
        "The code below prints the first ten labels contained in the y_train array. They represent the labels for the first 10 images of the train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKLOmhlwJQl",
        "colab_type": "code",
        "outputId": "2bd20c94-1d71-40b7-bed5-d792f11dbd5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqNSZOUoMvf6",
        "colab_type": "text"
      },
      "source": [
        "### Towards One-Hot ... ness??\n",
        "\n",
        "One hot vectors are zero vectors used to represent a set of natural numbers each belonging to a certain range named *num_classes*.\n",
        "\n",
        "So, if :\n",
        "\n",
        "- number = 3\n",
        "- num_classes = 10\n",
        "- one_hot_number = [0, 0, 0, 3, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "The *np.to_categorical* method expects input  of a numpy array in the shape *[batch_size]* and a positive integer called *nb_classes* and outputs a 2D array of shape *[batch_size, nb_classes]* where each element in the input *I[k]* in represented by the corensonding output row *O[k,:]*\n",
        "\n",
        "*PS : It apparently converts datatype to np.float32 as well*\n",
        "\n",
        "###Why one_hot at all?\n",
        "\n",
        "We use this encoding since it helps us calculate losses for all classes separately. Calculating losses in this manner helps us backpropagate gradients for errors in detecting all classes instead of just the input class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YusMJguiwKsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyspTYarQho7",
        "colab_type": "text"
      },
      "source": [
        "The code below prints the first 10 elements of Y_train which is the one-hot encoding of the y_train vector that represent the training labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upxc99AswMW0",
        "colab_type": "code",
        "outputId": "b7bd8647-73a7-4a48-8a87-3caf072a53b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z6tqR5ORau6",
        "colab_type": "text"
      },
      "source": [
        "###Defining the model\n",
        "\n",
        "The following code defines the architecture / model of our neural network. At this stage - we define the operations of the network *(i.e. adding the layers of the network to a pre-defined keras model)*\n",
        "\n",
        "- **Initializing a sequential model** : A Sequential model in keras stacks layers on top of one another. It makes sure that the input and outbut sizes of successive layers are compatible. In the first LOC below - we initialize an instance of the Sequential class.\n",
        "\n",
        "- **Adding convolution layers** : Now that we have initialized our model - we start adding layers. What we add to the sequential model are instances of the Convolution2D class defined in keras. While adding the first convolution layer we need to specify the input size *(height, width, num_channels)* as well. The next layers do not need an input size since they assume they will be in the shape of the preceeding layers model. We can also specify as an argument the activation to be used. *Default Stride = 1, pool_size = (3, 3), output_size = (batch_size, input_size - 2, output_size - 2, num_channels)*\n",
        "\n",
        "- **Adding Max Pooling layers** : Next, we add max pooling layers to reduct the first two dimentions of each image. This layer extracts the maximuim out of the *(2, 2)* pool and passes them forward - The output size would be *(input_size // 2, input_size // 2, num_channels)*\n",
        "\n",
        "- **Adding Flatten layers** : Finally, when we reach a GRF of the whole image *(1, 1, num_channels)*, we flatten the output *channel-wise* into an output of shape *(batch_size, 10)*. We also add a relu activation on top\n",
        "\n",
        "- **Adding the softmax layers** : Softmax layers implement a differentiable maximum function. Given a vector of elements *<v[1] ... v[n]>* if makes the following operation: *v[i] = e^(v[i]) / sum(e^v[k]) for all k in range [1,n]*. This converts the activations of the last layer into a probability distribution\n",
        "\n",
        "Following EVA conventions, we now note down the layers in the following manner:\n",
        "\n",
        "*batch_size --> b_s*\n",
        "\n",
        "- [b_s, 28, 28, 1] | Conv2D - [3, 3, 32] |  [b_s, 26, 26, 32],\n",
        "- [b_s, 26, 26, 32] | Conv2D - [3, 3, 64] | [b_s, 24, 24, 64]\n",
        "- [b_s, 24, 24, 64] | Conv2D - [3, 3, 128] | [b_s, 22, 22, 128]\n",
        "- [b_s, 22, 22, 128] | MaxPooling - [2, 2] | [b_s, 11, 11, 128]\n",
        "- [b_s, 11, 11, 128] | Conv2D - [3, 3, 256] | [b_s, 9, 9, 256]\n",
        "- [b_s, 9, 9, 256] | Conv2D - [3, 3, 512] | [b_s, 7, 7, 512]\n",
        "- [b_s, 7, 7, 512] | Conv2D - [3, 3, 1024] | [b_s, 5, 5, 1024]\n",
        "- [b_s, 5, 5, 1024] | Conv2D - [3, 3, 2048] | [b_s, 3, 3, 2048]\n",
        "- [b_s, 3, 3, 2048] | Conv2D - [3, 3, 10] | [b_s, 1, 1, 10]\n",
        "- [b_s, 1, 1, 10] | Flatten | [b_s, 10]\n",
        "- [b_s, 10] | Softmax | [b_s, 10]\n",
        "\n",
        "The discourse in the class still maintains that max pooling doubles the receptive feild - however - we can figure this out ourselves - by giving iot a little thought. \n",
        "\n",
        "[This image](https://github.com/anubhavsatpathy/EVA/blob/master/Session2/rf_max_pool.png) provides proof that a 2x2 max pool layer ups the GRF by 1 per dimention and thereafter, every 3x3 convolution layers increases the GRF by 4 - The comments in the block below follow this logic to list down the GRF values of the inputs in each layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irTVUE47wNwr",
        "colab_type": "code",
        "outputId": "fd80577d-cbda-4026-f375-d927de349aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        }
      },
      "source": [
        "from keras.layers import Activation, MaxPooling2D\n",
        "\n",
        "# All shapes are in the form [batch_size, height, width, num_channels]\n",
        "\n",
        "model = Sequential() \n",
        "\n",
        "# INPUT SIZE : [batch_size, 28, 28, 1] | OUTPUT SIZE : [batch_size, 26, 26, 32] | GRF_of_input : [1, 1]\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "# INPUT SIZE : [batch_size, 26, 26, 32] |  OUTPUT SIZE : [batch_size, 24, 24, 64] | GRF_of_input : [3, 3]\n",
        "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
        "# INPUT SIZE : [batch_size, 24, 24, 64] | OUTPUT SIZE : [batch_size, 22, 22, 128] | GRF_of_input : [5, 5]\n",
        "model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
        "# INPUT SIZE : [batch_size, 22, 22, 128] | OUTPUT SIZE : [batch_size, 11, 11, 128] | GRF_of_input : [7, 7]\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# INPUT SIZE : [batch_size, 11, 11, 128] | OUTPUT SIZE : [batch_size, 9, 9, 256] | GRF_of_input : [8, 8]\n",
        "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
        "# INPUT SIZE : [batch_size, 9, 9, 256] | OUTPUT SIZE : [batch_size, 7, 7, 512] | GRF_of_input : [12, 12]\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "# INPUT SIZE : [batch_size, 7, 7, 512] | OUTPUT SIZE : [batch_size, 5, 5, 1024] | GRF_of_input : [16, 16]\n",
        "model.add(Convolution2D(1024, 3, 3, activation='relu'))\n",
        "# INPUT SIZE : [batch_size, 5, 5, 1024] | OUTPUT SIZE : [batch_size, 3, 3, 2048] | GRF_of_input : [20, 20]\n",
        "model.add(Convolution2D(2048, 3, 3, activation='relu'))\n",
        "# INPUT SIZE : [batch_size, 3, 3, 2048] | OUTPUT SIZE : [batch_size, 1, 1, 10] | GRF_of_input : [24, 24]\n",
        "model.add(Convolution2D(10, 3, 3, activation='relu'))\n",
        "# INPUT SIZE : [batch_size, 1, 1, 10] | OUTPUT SIZE : [batch_size, 10] | GRF_of_input : [28, 28]\n",
        "model.add(Flatten())\n",
        "# INPUT SIZE : [batch_size, 10] | OUTPUT SIZE : [batch_size, 10] | GRF_of_input : [28, 28]\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_58 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_59 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_60 (Conv2D)           (None, 22, 22, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_61 (Conv2D)           (None, 9, 9, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv2d_62 (Conv2D)           (None, 7, 7, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_63 (Conv2D)           (None, 5, 5, 1024)        4719616   \n",
            "_________________________________________________________________\n",
            "conv2d_64 (Conv2D)           (None, 3, 3, 2048)        18876416  \n",
            "_________________________________________________________________\n",
            "conv2d_65 (Conv2D)           (None, 1, 1, 10)          184330    \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 25,348,362\n",
            "Trainable params: 25,348,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2048, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmZTnDX6Z7LS",
        "colab_type": "text"
      },
      "source": [
        "### Compiling the model:\n",
        "\n",
        "Compiling the model means that keras will actually create the computational graph using tensorflow backend. For this to happen, we need to provide a loss function and an optimizer and an accuracy metric. We define this as below:\n",
        "\n",
        "- **Loss Function** : A loss function is a mathematical function that calculates some sort of differentiable distance measure between the network's output and the actual output for each class. During training, we average out these losses and backpropagate them through the network. In this case - we use categorical cross-entropy\n",
        "\n",
        "- **Optimizer** : This step selects an optimizer algorithm that manipulates the learning rate per time-step so as to update the weights in a manner that does not lead to a swinging behaviour around the minimum. A few examples are *AdaGrad, Momentum etc*. but *Adam* has long been established as the king of the jungle\n",
        "\n",
        "- **Accuracy Metric** : This is a metric that measures the performance of the network. In the case of classification this is accuracy *(i.e. number_of_correct_predictions / number_of_total_examples_in_batch)*. Well there are other measures as well like BLEU score in case of machine translation etc.\n",
        "\n",
        "*model.compile() sets things up for us before we start training the model*\n",
        "\n",
        "###Categorical Cross Entropy:\n",
        "\n",
        "*Cross_Entropy = sum(-t[i] * log[l[i]] - (1 - t[i]) * log(1 - l[i]))*\n",
        "\n",
        "where :\n",
        "\n",
        "- *i* belongs to range *N[0, num_classes]*\n",
        "- *t[i]* --> prediction for class *i*\n",
        "- *l[i]* --> label for class *i*\n",
        "\n",
        "|t[i]   |v[i]   |CE[i]   |\n",
        "|---|---|---|\n",
        "|   1.0|   0.01|  4.55 |\n",
        "|   1.0|   0.05|   2.30|\n",
        "|   1.0|   0.95|   0.23|\n",
        "|   0.0|   0.01|   0.05|\n",
        "|   0.0|   0.5|   2.30|\n",
        "|   0.0|   0.95|   4.37|\n",
        "\n",
        "As can be seen in the table above - this decreases as the predictions come closer to labels and increases as they grow apart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYZOpRb6yG7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMT2_jMOwUVf",
        "colab_type": "text"
      },
      "source": [
        "### Training the model:\n",
        "\n",
        "Training examples are fed to a network in batches - this is called minibatch Gradient Descent instead of fullbatch Gradient Descent. There are subtle mathematical differences b/w minibatch and fullbatch versions but minibatch Gradient Descent has proved to be a winner in two aspects:\n",
        "\n",
        "- It consumes less memory and is operationally more efficient\n",
        "- It does not give up too much on the accuracy of the network\n",
        "\n",
        "Hence, we need a *batch_size* - we pass this as an argument to the *Sequential.fit()* method - In this case we pass *32*\n",
        "\n",
        "An *epoch* is the number of batches passed through the network before having covered the full training set.\n",
        "\n",
        "The *number of epoch* we want to train the network is again passed to the same method as an argument - In this case 10\n",
        "\n",
        "We also pass the training images and training labels to the fit method - The method breaks them down into batches and passes them through the network - calculates losses and backpropagates gradients for each batch.\n",
        "\n",
        "*FORM CURRENT BATCH --> PASS THROUGH THE NETWORK --> CALCULATE LOSSES --> BACKPROPAGATES GRADIENTS*\n",
        "\n",
        "It does the above functions for each batch in each epoch.\n",
        "\n",
        "X_train has to be in the shape *[num_examples, height, width, num_channels]*\n",
        "\n",
        "After training 60,000 images for 10 epochs using batch sizes of 32:\n",
        "\n",
        "- Losses stayed at around 2.30\n",
        "- Accuracy stayed at around 9%\n",
        "\n",
        "Wait what!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O248wVQyMft",
        "colab_type": "code",
        "outputId": "e8f09a6f-5044-40ac-c905-c60e519b180e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 117s 2ms/step - loss: 2.3028 - acc: 0.0986\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 115s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 2.3026 - acc: 0.0987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fca589f0400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sst4KneiyOL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfJiXOKsyj4y",
        "colab_type": "code",
        "outputId": "9146a116-67ed-4097-beaa-cf52bb7d0982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.3025851249694824, 0.098]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwLSXt7nyn_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWKKoOKwyppN",
        "colab_type": "code",
        "outputId": "f397f3f8-3c26-4de7-8c4a-86478c4d84c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9_YkZ_RbsN2",
        "colab_type": "text"
      },
      "source": [
        "###Not Ideal Indeed\n",
        "\n",
        "Since the losses are not decreasing aften 10 epochs and neither is the accuracy going up we can clearly say our network is upto no good. Why?\n",
        "\n",
        "- **Loss of information in the last layer** :  The last convolution layer brings down 2048 * 3 * 3 elements used to represent features of the image directly down to 10 * 1 * 1 features - thats a 99.945 % loss in information - no wonder we messed up. What happens if we undertake such a massive drop in information is that the gradients comin in get distributed across a ton of weights thus each weight is hardly if at all updated - Hence no learning\n",
        "\n",
        "- **ReLU in last layer** : Adding the ReLU activation in the last layer makes it much tougher for the network to learn class activations. Think of similar images like 5 and 2 that will have similar positive activations but the disparioty is contained in the negative activations - These disparities will dissappear after ReLU making it even tougher for the network to learn. (Disclaimer - This was added after the Thursday class wherein this was discussed)\n",
        "\n",
        "- **Lack of a bottleneck** : The max pooling layer should ideally be followed by a 1 * 1 convolution layer to shrink the number of channels and then begin the process of discriminating the next set of features in this case - the numbers themshelves\n",
        "\n",
        "- **Too many parameters** : 25,348,362 trainable parameters take a ton more time to train\n",
        "\n",
        "-  **Too many convolution** : We decreased the size of the last output of convolution to [1, 1] - This space is just too little to represent any information at all  about the numbers - remember convolutions detect information by representing them in the output layer using the features detected by the kernels - sizes like 3*3 and 1*1 just are not enough to represent these visual information about numbers\n",
        "\n",
        "- **Early Max-Pooling** : The max-pooling happens when the GRF is [7, 7] - most edges are visible at a receptive field of [9, 9] of [11, 11]. Well, I say that with a pinch of salt for the image size of this dataset is rather small\n",
        "\n",
        "- **Lack of normalization** : It is usually wise to normalize the layers after every few convolution layers - hence not ensuring that the activations are not flying all haywire\n",
        "\n",
        "We changed a few things about the network - let's see how it performs now:\n",
        "\n",
        "- **Training Accuracy** : 99.23% %\n",
        "- **Training Loss** : 0.03\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AslqWiTePhx1",
        "colab_type": "code",
        "outputId": "5fd11ee8-c874-41c7-fe2a-8475ddaba720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Activation, MaxPooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model1.add(Convolution2D(64, 3, 3, activation='relu'))\n",
        "model1.add(Convolution2D(128, 3, 3, activation='relu'))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model1.add(Convolution2D(32, 1, 1, activation = 'relu'))\n",
        "model1.add(Convolution2D(64, 3, 3, activation = 'relu'))\n",
        "model1.add(Convolution2D(128, 3, 3, activation = 'relu'))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Convolution2D(256, 3, 3, activation = 'relu'))\n",
        "model1.add(Convolution2D(128, 3, 3, activation = 'relu'))\n",
        "model1.add(Convolution2D(10, 3, 3, activation = 'relu'))\n",
        "model1.add(Flatten())\n",
        "model1.add(Activation('softmax'))\n",
        "model1.summary()\n",
        "\n",
        "model1.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model1.fit(X_train, Y_train, batch_size=100, nb_epoch=10, verbose=1)\n",
        "\n",
        "score = model1.evaluate(X_test, Y_test, verbose=1)\n",
        "print(score)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_110 (Conv2D)          (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_111 (Conv2D)          (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_112 (Conv2D)          (None, 22, 22, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 22, 22, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_113 (Conv2D)          (None, 11, 11, 32)        4128      \n",
            "_________________________________________________________________\n",
            "conv2d_114 (Conv2D)          (None, 9, 9, 64)          18496     \n",
            "_________________________________________________________________\n",
            "conv2d_115 (Conv2D)          (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_116 (Conv2D)          (None, 5, 5, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv2d_117 (Conv2D)          (None, 3, 3, 128)         295040    \n",
            "_________________________________________________________________\n",
            "conv2d_118 (Conv2D)          (None, 1, 1, 10)          11530     \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 791,914\n",
            "Trainable params: 791,402\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 15s 243us/step - loss: 0.8228 - acc: 0.6637\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 12s 201us/step - loss: 0.7647 - acc: 0.6774\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 12s 204us/step - loss: 0.7565 - acc: 0.6789\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 12s 204us/step - loss: 0.7481 - acc: 0.6806\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 12s 201us/step - loss: 0.7464 - acc: 0.6812\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 12s 200us/step - loss: 0.7016 - acc: 0.7011\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.4156 - acc: 0.8263\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.1120 - acc: 0.9564\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 12s 200us/step - loss: 0.0185 - acc: 0.9950\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 12s 201us/step - loss: 0.0164 - acc: 0.9956\n",
            "10000/10000 [==============================] - 2s 181us/step\n",
            "[0.03249157374362021, 0.9929]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}