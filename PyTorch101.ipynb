{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch101.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srirampattabiraman/Computer-vision/blob/master/PyTorch101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kYM3ylzYo1Q",
        "colab_type": "text"
      },
      "source": [
        "![PyTorch](https://devblogs.nvidia.com/wp-content/uploads/2017/04/pytorch-logo-dark.png)\n",
        "\n",
        "An open source machine learning framework that accelerates the path from research prototyping to production deployment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcVqSTrWY6oz",
        "colab_type": "text"
      },
      "source": [
        "# Tensor - Pytorch's core data structure\n",
        "\n",
        "In Python we can create lists, lists of lists, lists of lists and so on. In NumPy there is a `numpy.ndarray` which represents `n`- dimensional array. In math there is a special name for the generalization of vectors and matrices to a higher dimensional space - a tensor\n",
        "\n",
        "Tensor is an entity with a defined number of dimensions called an order (rank). \n",
        "\n",
        "**Scalar** can be considered as a rank-0-tensor. \n",
        "\n",
        "**Vector** can be introduced as a rank-1-tensor. \n",
        "\n",
        "**Matrices** can be considered as a rank-2-tensor.\n",
        "\n",
        "# Tensor Basics\n",
        "\n",
        "Let's import the torch module first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-oFVL2tYYqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGxuR5IEaFJa",
        "colab_type": "text"
      },
      "source": [
        "## Tensor Creation\n",
        "Let's view examples of matrices and tensors generation\n",
        "\n",
        "2-dimensional (rank-2) tensor of zeros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "897JQc25aD3E",
        "colab_type": "code",
        "outputId": "aa540030-356c-42d0-ccdf-0e22e593cbcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "torch.zeros(3, 4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pujmpEBhaPRA",
        "colab_type": "text"
      },
      "source": [
        "Random rank-3 tensor:\n",
        "_read the print below and convince yourself how this is a rank-3-tensor and learn what those 5, 3, 4 values are there for_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBrznTNhaOL7",
        "colab_type": "code",
        "outputId": "4056f5c9-01e8-49da-b030-e41ea94fc9a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "torch.rand(5, 3, 4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[8.3207e-02, 5.7105e-01, 8.4947e-01, 9.7555e-01],\n",
              "         [1.1677e-03, 2.2325e-01, 9.5255e-01, 9.3047e-01],\n",
              "         [8.5517e-01, 2.4974e-01, 1.4011e-01, 3.2691e-02]],\n",
              "\n",
              "        [[1.8025e-01, 7.3442e-01, 1.5170e-01, 5.3173e-01],\n",
              "         [8.2599e-01, 9.4925e-01, 1.3481e-01, 5.5524e-01],\n",
              "         [3.1359e-01, 3.7374e-01, 1.1995e-01, 6.2458e-01]],\n",
              "\n",
              "        [[2.7366e-03, 6.6127e-01, 9.5547e-01, 8.6089e-01],\n",
              "         [5.1423e-01, 9.6500e-01, 5.6237e-01, 3.9119e-01],\n",
              "         [9.8343e-01, 7.9998e-01, 5.3087e-01, 6.7127e-04]],\n",
              "\n",
              "        [[8.8795e-01, 6.5002e-01, 5.3035e-01, 6.1812e-01],\n",
              "         [8.3499e-01, 9.4921e-02, 6.1186e-01, 6.2292e-01],\n",
              "         [4.1491e-01, 3.4505e-01, 1.4967e-02, 1.0847e-01]],\n",
              "\n",
              "        [[8.1951e-01, 6.3924e-01, 3.2835e-01, 3.9459e-01],\n",
              "         [1.7525e-01, 7.5705e-01, 4.2486e-01, 2.8297e-01],\n",
              "         [6.9292e-01, 1.3675e-01, 4.6330e-01, 7.1258e-01]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO4fIr15asdi",
        "colab_type": "text"
      },
      "source": [
        "I am hoping you have noticed 4-elements in a row, 3 rows making one block and there are 5 blocks. \n",
        "\n",
        "Random rank-4-tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiCLvMGCaVZ3",
        "colab_type": "code",
        "outputId": "eb22dccf-2cc1-47fb-d4e4-a7d95d71ab57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "torch.rand(2, 2, 2, 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.0669, 0.4065, 0.2227],\n",
              "          [0.4745, 0.3195, 0.6534]],\n",
              "\n",
              "         [[0.7859, 0.5397, 0.6536],\n",
              "          [0.7184, 0.3628, 0.5841]]],\n",
              "\n",
              "\n",
              "        [[[0.9677, 0.0771, 0.8360],\n",
              "          [0.3148, 0.1170, 0.3941]],\n",
              "\n",
              "         [[0.2159, 0.0426, 0.3912],\n",
              "          [0.8062, 0.1351, 0.0943]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF6VL68s3AMV",
        "colab_type": "text"
      },
      "source": [
        "## Question 1:\n",
        "\n",
        "How many dimensions are there in a tensor defined as below?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IbXcI3x3Ibt",
        "colab_type": "code",
        "outputId": "3ac886bb-d633-4164-c66e-3a665698a150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.rand(1, 1, 1, 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.4129]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmgpqssRa9jF",
        "colab_type": "text"
      },
      "source": [
        "4\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "There are many more ways to create tensor using some restrictions on values it should contatn - for the full reference, please follow the [official docs](https://pytorch.org/docs/stable/torch.html#creation-ops). \n",
        "\n",
        "\n",
        ".\n",
        "---\n",
        "\n",
        "\n",
        "# Python / NumPy / Pytorch interoperability\n",
        "\n",
        "You can create tensors from python as well as numpy arrays. You can also convert torch tensors to numpy arrays. So, the interoperability between torch and numpy is pretty good. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipgpeWPfa6gE",
        "colab_type": "code",
        "outputId": "d46aaf69-90ae-4a70-ad09-8ce4c045f945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Simple Python List\n",
        "python_list = [1, 2]\n",
        "\n",
        "# Create a numpy array from python list\n",
        "numpy_array = np.array(python_list)\n",
        "\n",
        "# Create a torch Tensor from python list\n",
        "tensor_from_list = torch.tensor(python_list)\n",
        "\n",
        "# Create a torch Tensor from Numpy array\n",
        "tensor_from_array = torch.tensor(numpy_array)\n",
        "\n",
        "# Another way to create a torch Tensor from Numpy array (share same storage)\n",
        "tensor_from_array_v2 = torch.from_numpy(numpy_array)\n",
        "\n",
        "# Convert torch tensor to numpy array\n",
        "array_from_tensor = tensor_from_array.numpy()\n",
        "\n",
        "print('List:   ', python_list)\n",
        "print('Array:  ', numpy_array)\n",
        "print('Tensor: ', tensor_from_list)\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Tensor: ', tensor_from_array_v2)\n",
        "print('Array:  ', array_from_tensor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List:    [1, 2]\n",
            "Array:   [1 2]\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([1, 2])\n",
            "Array:   [1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_x-86B8gqik",
        "colab_type": "text"
      },
      "source": [
        "**Difference between** `torch.Tensor` **and** `torch.from_numpy`\n",
        "\n",
        "Pytorch aims to be an effective library for computations. What does it mean? It means that pytorch avoids memory copying if it can. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHCKDGpygn_d",
        "colab_type": "code",
        "outputId": "f8112d84-3846-4bb4-8063-6033f3959dfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "numpy_array[0] = 10\n",
        "\n",
        "print('Array:  ', numpy_array)\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Tensor: ', tensor_from_array_v2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Array:   [10  2]\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([10,  2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CKtdNRM3SMp",
        "colab_type": "text"
      },
      "source": [
        "## Question 2:\n",
        "\n",
        "Assume that we moved our complete (cats vs dogs) image dataset to numpy arrays. Then we use torch.from_numpy to convert these images to tensor. Then we apply a specific data augmentation strategy called \"CutOut\" which blocks a portion of the image directly on these tensors. What will happen to the accuracy of a model trained on this strategy compared to the one without this strategy? CutOut strategy is shown below: \n",
        "\n",
        "![CutOut](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSnSyN835AmtQPKQbPjDHX-FmshNilbtexX95cRGQPwl56QCGDn)\n",
        "\n",
        "## Question 3:\n",
        "Why do you think we are observing this behavior?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekJldZ2yLaSV",
        "colab_type": "text"
      },
      "source": [
        "Answer 2: The validation accuracy of the model will be increased.\n",
        "\n",
        "Answer 3 : As instead of random removing of pixels or kernels in dropout on intermediate layers we are creating visual inconsistency in image itslef that makes the network robust to noisy inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8pIoPTNLWek",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZq7O-aihSOA",
        "colab_type": "text"
      },
      "source": [
        "We have two different ways to create tensor from its NumPy counterpart - one copies memory and another one shares the same underlying storage. It works in the opposite way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNFOwV8EhPwQ",
        "colab_type": "code",
        "outputId": "f276550b-32c1-4486-9f7a-fa2555a03054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "array_from_tensor = tensor_from_array.numpy()\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Array: ', array_from_tensor)\n",
        "\n",
        "tensor_from_array[1] = 11\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Array: ', array_from_tensor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor:  tensor([1, 2])\n",
            "Array:  [1 2]\n",
            "Tensor:  tensor([ 1, 11])\n",
            "Array:  [ 1 11]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM9ytbvKhtCw",
        "colab_type": "text"
      },
      "source": [
        "## Data types\n",
        "\n",
        "The basic data type of all Deep Learning-related operations is float, but sometimes you may need something else. Pytorch support different number types for its tensors the same way NumPy does it - by specifying the data type on tensor creation or via casting. Ths full list of supported data types can be found [here](https://pytorch.org/docs/stable/tensors.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd6WkzJ4hpYi",
        "colab_type": "code",
        "outputId": "694c3afd-73c8-44ad-8c18-b627867fa3c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "tensor = torch.zeros(2, 2)\n",
        "print('Tensor with default type: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.float16)\n",
        "print('Tensor with 16-bit float: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.int16)\n",
        "print('Tensor with integers: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.bool)\n",
        "print('Tensor with boolean data: ', tensor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor with default type:  tensor([[0., 0.],\n",
            "        [0., 0.]])\n",
            "Tensor with 16-bit float:  tensor([[0., 0.],\n",
            "        [0., 0.]], dtype=torch.float16)\n",
            "Tensor with integers:  tensor([[0, 0],\n",
            "        [0, 0]], dtype=torch.int16)\n",
            "Tensor with boolean data:  tensor([[False, False],\n",
            "        [False, False]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9F4Dkdr40TE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Question 4:\n",
        "We saw above that some times numpy and tensors share same storage and changing one changes the other. \n",
        "If we define a rank-2-tensor with ones (dtype of f16), and then convert it into a numpy data type using tensor.numpy() and store it in a variable called \"num\", and then we perform this operation `num = num * 0.5`, will the original tensor have 1.0s or 0.5s as its element values? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSTYwLr63e0n",
        "colab_type": "text"
      },
      "source": [
        "1 As it uses tensor.numpy() that refers memory not storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiEIKiMC4s9g",
        "colab_type": "code",
        "outputId": "c21cd7e5-0b2b-4b52-80bd-9c795693f267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "num = torch.rand(1,1)\n",
        "print(num)\n",
        "num_np = num.numpy()\n",
        "num_np[:] = num_np * 0.5\n",
        "num"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.3092]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1546]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiQt8Mmm51OE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 5: \n",
        "If the operation `num = num*5` is changed to `num[:] = num*5` will the original tensor have 1.0s or 0.5s as its element values? \n",
        "\n",
        "\n",
        "Neither as it multiplies \"num\" with 5 it has 5 as its elements value\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzh8UB8KiVmb",
        "colab_type": "text"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "Tensor provides access to its elements via the same `[]` operation as a regular python list or NumPy array. However, as you may recall from NumPy usage, the full power of math libraries is accessible only via vectorized operations, i.e. operations without explicit looping over all vector elements in python and using implicit optimized loops in C/C++/CUDA/Fortran/etc. available via special function calls. Pytorch employs the same paradigm and provides a wide range of vectorized operations. Let's take a look at some examples. \n",
        "\n",
        "Joining a list of tensors together with `torch.cat`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMaCDKPhiUAb",
        "colab_type": "code",
        "outputId": "d53b8460-6e76-4c94-ce56-a600e8cd38c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "a = torch.zeros(3, 2)\n",
        "b = torch.ones(3, 2)\n",
        "\n",
        "print(torch.cat((a, b), dim=1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bj4aeE86zdH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 6: \n",
        "Is the transpose of concatenated a & b tensor on dimension 1, same as the contatenated tensor of a & b on dimension 0? \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkbjw-LE-Cb3",
        "colab_type": "text"
      },
      "source": [
        "yes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TXNne69j7LP",
        "colab_type": "text"
      },
      "source": [
        "Indexing with another tensor/array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiE-Fsi4jVd6",
        "colab_type": "code",
        "outputId": "5a7bb013-d248-423e-bc75-c96d9de7cbd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "a = torch.arange(start=0, end=10)\n",
        "indices = np.arange(0, 10) > 5\n",
        "print(a)\n",
        "print(indices)\n",
        "print(a[indices])\n",
        " \n",
        "\n",
        "indices = torch.arange(start=0, end=10) %5\n",
        "print(indices)\n",
        "print(a[indices])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "[False False False False False False  True  True  True  True]\n",
            "tensor([6, 7, 8, 9])\n",
            "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n",
            "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS4dnlu47WQu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 7:\n",
        "\n",
        "`a` is defined as `torch.arange(start=0, end=10)`. We will create `b` using the two operations as below. In both cases do we get the same value?\n",
        "\n",
        "\n",
        "1.   indices variable created by the modulo operation on arange between 0 and 10. Then a new varialble `b` is created from `a` using the last 5 elements of indices. \n",
        "2.   indices variable created by the modulo operation on arange betwenn 1 and 11. Then a new varialble `b` is created from `a` using the last 5 elements of indices. \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD0nKzCv_r-d",
        "colab_type": "text"
      },
      "source": [
        "No-\n",
        "Case 1: [5,6,7,8,9]\n",
        "Case 2: [6,7,8,9,10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPyyZ2kf_ThP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ4ZCVsTk-KH",
        "colab_type": "text"
      },
      "source": [
        "What should we do if we have, say, rank-2-tensor and want to select only some rows?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GtRpotjkt1q",
        "colab_type": "code",
        "outputId": "abb3bdb9-63f2-461f-b690-9ae5d05488c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "tensor = torch.rand((5, 3))\n",
        "rows = torch.tensor([0, 2])\n",
        "print(tensor)\n",
        "tensor[rows]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.7193, 0.2334, 0.9437],\n",
            "        [0.0222, 0.1691, 0.6466],\n",
            "        [0.2249, 0.5834, 0.3149],\n",
            "        [0.4965, 0.6550, 0.9493],\n",
            "        [0.7989, 0.0809, 0.8687]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7193, 0.2334, 0.9437],\n",
              "        [0.2249, 0.5834, 0.3149]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIJnr_N2_Qaf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 8: \n",
        "\n",
        "Consider a tensor defined as `torch.rand((6, 5))`. Is the shape of the new tensor created by taking the 0th, 2nd and 4th row of the old tensor same as the shape of the a newer tensor created by taking the 0th, 2nd and 4th row of the old tensor after transposing it by operation `torch.transpose(tensor, 0, 1)` ?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_5NNYGQCBAE",
        "colab_type": "text"
      },
      "source": [
        "Yes as the transpose of 3 cross 3  is also  3 cross 3 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naDzFMkslU0b",
        "colab_type": "text"
      },
      "source": [
        "## Tensor Shapes\n",
        "\n",
        "Reshaping a tensor is a frequently used operation. We can change the shape of a tensor without the memory copying overhead. There are two methods for that: `reshape` and `view`. \n",
        "\n",
        "The difference is the following: \n",
        "\n",
        "\n",
        "*   view tries to return a tensor, and it shares the same memory with the original tensor. In case, if it cannot reuse the same memory due to [some reason](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view), it just fails. \n",
        "*   reshape always returns the tensor with the desired shape and tries to reuse the memory. If it cannot, it creates a copy\n",
        "\n",
        "Let's see with the help of an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HClDkqLLlMJh",
        "colab_type": "code",
        "outputId": "6ef72dad-b914-412f-cfc0-99a283780a4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "tensor = torch.rand(2, 3, 4)\n",
        "print('Pointer to data: ', tensor.data_ptr())\n",
        "print('Shape: ', tensor.shape)\n",
        "\n",
        "reshaped = tensor.reshape(24)\n",
        "\n",
        "view = tensor.view(3, 2, 4)\n",
        "print('Reshaped tensor - pointer to data', reshaped.data_ptr())\n",
        "print('Reshaped tensor shape ', reshaped.shape)\n",
        "\n",
        "print('Viewed tensor - pointer to data', view.data_ptr())\n",
        "print('Viewed tensor shape ', view.shape)\n",
        "\n",
        "assert tensor.data_ptr() == view.data_ptr()\n",
        "\n",
        "assert np.all(np.equal(tensor.numpy().flat, reshaped.numpy().flat))\n",
        "\n",
        "print('Original stride: ', tensor.stride())\n",
        "print('Reshaped stride: ', reshaped.stride())\n",
        "print('Viewed stride: ', view.stride())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pointer to data:  101358848\n",
            "Shape:  torch.Size([2, 3, 4])\n",
            "Reshaped tensor - pointer to data 101358848\n",
            "Reshaped tensor shape  torch.Size([24])\n",
            "Viewed tensor - pointer to data 101358848\n",
            "Viewed tensor shape  torch.Size([3, 2, 4])\n",
            "Original stride:  (12, 4, 1)\n",
            "Reshaped stride:  (1,)\n",
            "Viewed stride:  (8, 4, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIN5jSppm4yC",
        "colab_type": "text"
      },
      "source": [
        "The basic rule about reshaping the tensor is definitely that you cannot change the total number of elements in it, so the product of all tensor's dimensions should always be the same. It gives us the ability to avoid specifying one dimension when reshaping the tensor - Pytorch can calculate it for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3D19ERFmzOl",
        "colab_type": "code",
        "outputId": "10f855d5-24ff-4e47-ddad-3e7037a1d24f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "print(tensor.reshape(3, 2, 4).shape)\n",
        "print(tensor)\n",
        "print(tensor.reshape(3, 2, -1).shape)\n",
        "print(tensor.reshape(3, -1, 4).shape)\n",
        "print(tensor.reshape(2, 3, 4).shape)\n",
        "print(tensor.reshape(-1).shape)\n",
        "print(tensor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 2, 4])\n",
            "tensor([[[0.8679, 0.5687, 0.1801, 0.6781],\n",
            "         [0.2998, 0.5073, 0.2777, 0.2601],\n",
            "         [0.0617, 0.8595, 0.7866, 0.7627]],\n",
            "\n",
            "        [[0.7937, 0.8857, 0.6163, 0.5771],\n",
            "         [0.9261, 0.0214, 0.7028, 0.1098],\n",
            "         [0.5903, 0.7435, 0.0591, 0.9084]]])\n",
            "torch.Size([3, 2, 4])\n",
            "torch.Size([3, 2, 4])\n",
            "torch.Size([2, 3, 4])\n",
            "torch.Size([24])\n",
            "tensor([[[0.8679, 0.5687, 0.1801, 0.6781],\n",
            "         [0.2998, 0.5073, 0.2777, 0.2601],\n",
            "         [0.0617, 0.8595, 0.7866, 0.7627]],\n",
            "\n",
            "        [[0.7937, 0.8857, 0.6163, 0.5771],\n",
            "         [0.9261, 0.0214, 0.7028, 0.1098],\n",
            "         [0.5903, 0.7435, 0.0591, 0.9084]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szm9tJEHFY-2",
        "colab_type": "code",
        "outputId": "5bc3c407-8a67-42b3-ab29-cc76330e1027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "a = torch.tensor([1, 2, 3])\n",
        "# print(a.reshape(-1, 2).shape)\n",
        "a = a.reshape(-1, 3)\n",
        "print(a.shape)\n",
        "b = torch.tensor([[2, 1]])\n",
        "b = b.view(2, -1)\n",
        "print(b.shape)\n",
        "d = torch.mm(a, b)\n",
        "# b = torch.rand(2,1)\n",
        "# # b = b.reshape(1,2)\n",
        "# b = b.view(2, -1)\n",
        "# print(b)\n",
        "# odt = torch.mm(a, b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3])\n",
            "torch.Size([2, 1])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-09ff9ed458e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# b = torch.rand(2,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# # b = b.reshape(1,2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 3], m2: [2 x 1] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:136"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObgCQKUiETak",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Question 9:\n",
        "\n",
        "Consider a tensor `a` created with [1, 2, 3] and [1, 2, 3] of size (2, 3) is reshaped with operation `.reshape(-1, 2)`. Also consider a tensor `b` created with [[2, 1]] and of size (1, 2), later operated with `view(2, -1)` operation. \n",
        "\n",
        "If we do a dot product of a and b (using `torch.mm`) and perform the sum of all the elements (using `torch.sum`) what do we get? (enter int value without any decimal point in the quiz)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C038B3wCHTr7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mza7QPg3ndeV",
        "colab_type": "text"
      },
      "source": [
        "**Alternative ways to view tensors** - `expand` or `expand_as`.\n",
        "\n",
        "\n",
        "\n",
        "*   `expand` - requires the desired shape as an input\n",
        "*   `expand_as` - uses the shape of another tensor\n",
        "\n",
        "These operations \"repeat\" tensor's values along the specified axes without actually copying the data. \n",
        "\n",
        "As the documentation says, expand:\n",
        "\n",
        "\n",
        "> returns a new view of the self tensor with singleton dimensions expanded to a larger size. Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1. \n",
        "\n",
        "**Use case:**\n",
        "\n",
        "\n",
        "\n",
        "*   index multi-channel tensor with single-channel mask - imagine a color image with 3 channels (RGB) and binary mask for the area of interest on that image. We cannot index the image with this kind of mask directly since the dimensions are different, but we can use `expand_as` operation to create a view of the mask that has the same dimensions as the image we want to apply it to, but has not copied the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz33E-V7nPQT",
        "colab_type": "code",
        "outputId": "b97959af-126b-4d6c-98fa-d3915f99fd78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Create a black image\n",
        "image = torch.zeros(size=(3, 256, 256), dtype=torch.int)\n",
        "\n",
        "# Leave the borders and make the rest of the image Green\n",
        "image[1, 18:256 - 18, 18:256 - 18] = 255\n",
        "\n",
        "# Create a mask of the same size\n",
        "mask = torch.zeros(size=(256, 256), dtype=torch.bool)\n",
        "\n",
        "\n",
        "# Assuming the green region in the original image is the Region of interest, change the mask to white for that area\n",
        "mask[18:256 - 18, 18:256 - 18] = 1\n",
        "# print(mask)\n",
        "\n",
        "# Create a view of the mask with the same dimensions as the original image\n",
        "mask_expanded = mask.expand_as(image)\n",
        "# print(mask_expanded)\n",
        "print(mask_expanded.shape)\n",
        "\n",
        "mask_np = mask_expanded.numpy().transpose(1, 2, 0) * 255\n",
        "image_np = image.numpy().transpose(1, 2, 0)\n",
        "# print(image_np)\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()\n",
        "\n",
        "image[0, mask] += 128\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()\n",
        "\n",
        "image[mask_expanded] += 128\n",
        "image.clamp_(0, 255)\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 256, 256])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiXBx0k3ptOI",
        "colab_type": "text"
      },
      "source": [
        "In the example above, one can also find a couple of useful tricks:\n",
        "\n",
        "\n",
        "*   `clamp` method and function is a Pytorch's analogue of NumPy's `clip` function\n",
        "*   many operations on tensors have in-place form, that does not return modified data, but change values in the tensor. The in-place version of the operation has trailing underscore according to Pytorch's naming convension - in the exmaple above it is `clamp_`\n",
        "*   tensors have the same indexing as Numpy's arrays - one can use `:` seperated range, negative indexes and so on.\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Images and their representations\n",
        "\n",
        "Now, let's discuss images, their representations and how different Python librarties work with them. \n",
        "\n",
        "Probably, the most well-known library for image loading and simple processing is [Pillow](https://pillow.readthedocs.io/en/stable/). \n",
        "\n",
        "However, many people in deep learning area stick with OpenCV for image loading and processing with some usage of another libraries when it is justified by performance/functionality. This is because OpenCV is in general much faster than the other libraries. Here you can find a couple of benchmarks: \n",
        "\n",
        "*   https://www.kaggle.com/zfturbo/benchmark-2019-speed-of-image-reading\n",
        "*   https://github.com/albumentations-team/albumentations#benchmarking-results\n",
        "\n",
        "To sum up the benchmarks above, there are two most common image formats, PNG and JPEGs. If your data is in PNG format - use OpenCV to read it. If it is in JPEG - use libturbojpeg. For image processing, use OpenCV if possible. _We will be using PIL a lot along with these._\n",
        "\n",
        "As you will read the code from others, you may find out that some of them use Pillow/something else to read data. You should know, that color image representations in OpenCV and other libraries are different - OpenCV uses \"BGR\" channel order, while others use \"RGB\" one. \n",
        "\n",
        "To change \"BRG\" <-> \"RGB\" the only thing we need to do it to change channel order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYv4sZMmpndu",
        "colab_type": "code",
        "outputId": "449b97a7-7201-4ef5-9a09-0f278369131d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "print(os.getcwd())\n",
        "\n",
        "bgr_image = cv2.imread('./mars.jpeg') \n",
        "# remember to add your own image in case you run this block, if you want to use the same image, \n",
        "# download it from: https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRCA40ftnscVzfV8ft8e7vIzQXfXeZdtco8nknJrfCUW6INI40U\n",
        "rgb_image = bgr_image[..., ::-1]\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(bgr_image)\n",
        "ax[1].imshow(rgb_image)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAACHCAYAAADtJRlTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoi4TeNBGHle",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 10:\n",
        "\n",
        "Looking at the results above it can be said that the pixel values in the blue channels would be very small compared to red channel. True/False?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTULLUcxQOBr",
        "colab_type": "text"
      },
      "source": [
        "false"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ugFd57zNwa",
        "colab_type": "text"
      },
      "source": [
        "# Autograd\n",
        "\n",
        "Pytorch supports automatic differentiation. The module which implements this is called **AutoGrad**. It calculates the gradients and keeps track in forward and backward passes. For primitive tensors, you need to enable or disable it using the `required_grad` flag. But, for advanced tensors, it is enabled by default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMOp4aiou6JR",
        "colab_type": "code",
        "outputId": "4fafa3f8-f8d2-4f35-ead1-58190fda1852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad = True)\n",
        "print(a)\n",
        "result = a * 5\n",
        "print(result)\n",
        "\n",
        "# grad can be implicitly created only for scalar outputs\n",
        "# so let's calculate the sum here so that the output becomes a scalar and we can apply a backward pass\n",
        "mean_result = result.sum()\n",
        "print(mean_result)\n",
        "# calculate gradient\n",
        "mean_result.backward()\n",
        "# print gradient of a\n",
        "print(a.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1365, 0.7906, 0.2476, 0.3888, 0.1967],\n",
            "        [0.6184, 0.7177, 0.7143, 0.5078, 0.2485],\n",
            "        [0.6969, 0.8610, 0.2125, 0.9390, 0.0471]], requires_grad=True)\n",
            "tensor([[0.6823, 3.9530, 1.2378, 1.9438, 0.9833],\n",
            "        [3.0921, 3.5883, 3.5716, 2.5390, 1.2426],\n",
            "        [3.4844, 4.3048, 1.0624, 4.6950, 0.2357]], grad_fn=<MulBackward0>)\n",
            "tensor(36.6162, grad_fn=<SumBackward0>)\n",
            "tensor([[5., 5., 5., 5., 5.],\n",
            "        [5., 5., 5., 5., 5.],\n",
            "        [5., 5., 5., 5., 5.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym0Amk2IGfLx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 11: \n",
        "\n",
        "Why the gradient of a is all 5s above?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqQpIBc_RLHr",
        "colab_type": "text"
      },
      "source": [
        "differentiation of 5a with respect to a gives 5 as result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PDgGq2R0k7I",
        "colab_type": "text"
      },
      "source": [
        "As we see, Pytorch automagically calculated the gradient value for us. It looks to be the correct value - we multiplied an input by 5, so the gradient of this operation equals to 5.\n",
        "\n",
        "# Disabling Autograd for tensors\n",
        "\n",
        "We don't need to compute gradients for all the variables that are involved in the pipeline. The Pytorch API provides 2 ways to disable autograd.\n",
        "\n",
        "`detach` - returns a copy of the tensor with autograd disabled. This \n",
        "\n",
        "1.   copy is built on the same memory as the original tensor, so in-place size / stride / storage changes (such as resize_ / resizeas / set / transpose) modifications are not allowed.\n",
        "2.   torch.no_grad() - It is a context manager that allows you to guard a series of operations from autograd without creating new tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqVG9fQb0cLW",
        "colab_type": "code",
        "outputId": "2081f403-31a2-4296-c670-b899e246a8cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad=True)\n",
        "detached_a = a.detach()\n",
        "detached_result = detached_a * 5\n",
        "result = a * 10\n",
        "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
        "# so let's calculate the sum here\n",
        "mean_result = result.sum()\n",
        "mean_result.backward()\n",
        "a.grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqpch2Be02J7",
        "colab_type": "code",
        "outputId": "0a5ee8e8-81b8-46f0-c7fc-ac406657e928",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad=True)\n",
        "with torch.no_grad():\n",
        "    detached_result = a * 5\n",
        "result = a * 10\n",
        "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
        "# so let's calculate the sum here\n",
        "mean_result = result.sum()\n",
        "mean_result.backward()\n",
        "a.grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjh2rYOPJUAZ",
        "colab_type": "text"
      },
      "source": [
        "# Custom Network\n",
        "\n",
        "A fully-connected ReLU network with one hidden layer and no biases, trained to predict y from x by minimizing squared Euclidean distance.\n",
        "\n",
        "This implementation uses PyTorch tensors to manually compute the forward pass, loss, and backward pass.\n",
        "\n",
        "A PyTorch Tensor is basically the same as a numpy array: it does not know anything about deep learning or computational graphs or gradients, and is just a generic n-dimensional array to be used for arbitrary numeric computation.\n",
        "\n",
        "The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU, just cast the Tensor to a cuda datatype."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nf5RaB104Vp",
        "colab_type": "code",
        "outputId": "e2e1688f-49fb-4bc1-a2a1-99bf393b87eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y\n",
        "    h = x.mm(w1)\n",
        "    h_relu = h.clamp(min=0)\n",
        "    y_pred = h_relu.mm(w2)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    grad_y_pred = 2*(y_pred - y)\n",
        "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
        "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
        "    grad_h = grad_h_relu.clone()\n",
        "    grad_h[h < 0] = 0\n",
        "    grad_h_relu[h < 0] = 0\n",
        "    grad_w1 = x.t().mm(grad_h_relu)\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 27818666.0\n",
            "1 21826338.0\n",
            "2 19356052.0\n",
            "3 17713342.0\n",
            "4 15534900.0\n",
            "5 12737260.0\n",
            "6 9574709.0\n",
            "7 6761763.0\n",
            "8 4559550.5\n",
            "9 3044367.75\n",
            "10 2052183.25\n",
            "11 1427463.125\n",
            "12 1031567.125\n",
            "13 777255.8125\n",
            "14 608073.4375\n",
            "15 490791.9375\n",
            "16 405862.6875\n",
            "17 341880.65625\n",
            "18 291897.625\n",
            "19 251800.46875\n",
            "20 218894.921875\n",
            "21 191432.25\n",
            "22 168226.546875\n",
            "23 148431.96875\n",
            "24 131447.34375\n",
            "25 116785.4375\n",
            "26 104059.203125\n",
            "27 92962.609375\n",
            "28 83244.921875\n",
            "29 74705.5859375\n",
            "30 67182.765625\n",
            "31 60530.8828125\n",
            "32 54636.671875\n",
            "33 49402.73828125\n",
            "34 44744.35546875\n",
            "35 40585.3515625\n",
            "36 36866.6484375\n",
            "37 33534.609375\n",
            "38 30544.85546875\n",
            "39 27856.546875\n",
            "40 25436.38671875\n",
            "41 23256.974609375\n",
            "42 21288.35546875\n",
            "43 19507.375\n",
            "44 17893.7109375\n",
            "45 16429.078125\n",
            "46 15098.927734375\n",
            "47 13888.91796875\n",
            "48 12787.421875\n",
            "49 11782.6669921875\n",
            "50 10865.6591796875\n",
            "51 10027.53515625\n",
            "52 9261.3271484375\n",
            "53 8560.322265625\n",
            "54 7917.9150390625\n",
            "55 7328.44287109375\n",
            "56 6787.1611328125\n",
            "57 6289.78466796875\n",
            "58 5832.6083984375\n",
            "59 5411.93408203125\n",
            "60 5024.30810546875\n",
            "61 4666.8828125\n",
            "62 4337.1357421875\n",
            "63 4032.898681640625\n",
            "64 3751.967041015625\n",
            "65 3492.045166015625\n",
            "66 3251.98779296875\n",
            "67 3029.89306640625\n",
            "68 2824.15283203125\n",
            "69 2633.525146484375\n",
            "70 2456.80029296875\n",
            "71 2292.916748046875\n",
            "72 2140.734375\n",
            "73 1999.357666015625\n",
            "74 1868.0201416015625\n",
            "75 1745.956298828125\n",
            "76 1632.4249267578125\n",
            "77 1526.8427734375\n",
            "78 1428.477294921875\n",
            "79 1336.896484375\n",
            "80 1251.587890625\n",
            "81 1172.073486328125\n",
            "82 1097.992919921875\n",
            "83 1028.859130859375\n",
            "84 964.3659057617188\n",
            "85 904.156982421875\n",
            "86 847.9173583984375\n",
            "87 795.3919677734375\n",
            "88 746.2904663085938\n",
            "89 700.4086303710938\n",
            "90 657.49658203125\n",
            "91 617.3671875\n",
            "92 579.8076782226562\n",
            "93 544.6468505859375\n",
            "94 511.73419189453125\n",
            "95 480.9147644042969\n",
            "96 452.02978515625\n",
            "97 424.9540710449219\n",
            "98 399.5775451660156\n",
            "99 375.7830810546875\n",
            "100 353.4787292480469\n",
            "101 332.5516662597656\n",
            "102 312.92572021484375\n",
            "103 294.5095520019531\n",
            "104 277.2163391113281\n",
            "105 260.9779968261719\n",
            "106 245.7346954345703\n",
            "107 231.4190216064453\n",
            "108 217.96600341796875\n",
            "109 205.3248291015625\n",
            "110 193.4458770751953\n",
            "111 182.2875518798828\n",
            "112 171.79052734375\n",
            "113 161.91989135742188\n",
            "114 152.63818359375\n",
            "115 143.90240478515625\n",
            "116 135.6852569580078\n",
            "117 127.95568084716797\n",
            "118 120.6791000366211\n",
            "119 113.83113098144531\n",
            "120 107.38220977783203\n",
            "121 101.3084945678711\n",
            "122 95.59346771240234\n",
            "123 90.20846557617188\n",
            "124 85.13833618164062\n",
            "125 80.35862731933594\n",
            "126 75.8528060913086\n",
            "127 71.60997772216797\n",
            "128 67.60973358154297\n",
            "129 63.83876037597656\n",
            "130 60.28308868408203\n",
            "131 56.9292106628418\n",
            "132 53.769046783447266\n",
            "133 50.78733825683594\n",
            "134 47.97423553466797\n",
            "135 45.320491790771484\n",
            "136 42.819114685058594\n",
            "137 40.46030807495117\n",
            "138 38.23171615600586\n",
            "139 36.128883361816406\n",
            "140 34.14399719238281\n",
            "141 32.27052688598633\n",
            "142 30.50282859802246\n",
            "143 28.833463668823242\n",
            "144 27.257265090942383\n",
            "145 25.768625259399414\n",
            "146 24.363386154174805\n",
            "147 23.036285400390625\n",
            "148 21.781906127929688\n",
            "149 20.598052978515625\n",
            "150 19.478042602539062\n",
            "151 18.42178726196289\n",
            "152 17.42306137084961\n",
            "153 16.479394912719727\n",
            "154 15.587510108947754\n",
            "155 14.745064735412598\n",
            "156 13.948548316955566\n",
            "157 13.195979118347168\n",
            "158 12.48507022857666\n",
            "159 11.812398910522461\n",
            "160 11.176776885986328\n",
            "161 10.575799942016602\n",
            "162 10.007318496704102\n",
            "163 9.470012664794922\n",
            "164 8.961828231811523\n",
            "165 8.481260299682617\n",
            "166 8.027066230773926\n",
            "167 7.597582817077637\n",
            "168 7.191204071044922\n",
            "169 6.807062149047852\n",
            "170 6.443080902099609\n",
            "171 6.0995774269104\n",
            "172 5.774292469024658\n",
            "173 5.4665093421936035\n",
            "174 5.175572872161865\n",
            "175 4.900417804718018\n",
            "176 4.639641284942627\n",
            "177 4.393100738525391\n",
            "178 4.1595587730407715\n",
            "179 3.938516616821289\n",
            "180 3.7295758724212646\n",
            "181 3.531874418258667\n",
            "182 3.344923496246338\n",
            "183 3.167705535888672\n",
            "184 2.9999167919158936\n",
            "185 2.8411083221435547\n",
            "186 2.6908440589904785\n",
            "187 2.548616409301758\n",
            "188 2.4141905307769775\n",
            "189 2.2866435050964355\n",
            "190 2.1658973693847656\n",
            "191 2.0516834259033203\n",
            "192 1.9434938430786133\n",
            "193 1.8410242795944214\n",
            "194 1.744102120399475\n",
            "195 1.6521844863891602\n",
            "196 1.5651373863220215\n",
            "197 1.4828897714614868\n",
            "198 1.4048447608947754\n",
            "199 1.331099271774292\n",
            "200 1.2612428665161133\n",
            "201 1.194861888885498\n",
            "202 1.132145881652832\n",
            "203 1.0727639198303223\n",
            "204 1.0165143013000488\n",
            "205 0.9632779359817505\n",
            "206 0.9126571416854858\n",
            "207 0.8649839162826538\n",
            "208 0.819608747959137\n",
            "209 0.776713490486145\n",
            "210 0.7361165881156921\n",
            "211 0.6975268125534058\n",
            "212 0.6612092852592468\n",
            "213 0.6264442801475525\n",
            "214 0.5937885046005249\n",
            "215 0.5627316236495972\n",
            "216 0.5333472490310669\n",
            "217 0.5055290460586548\n",
            "218 0.4791606068611145\n",
            "219 0.4541161060333252\n",
            "220 0.4304726719856262\n",
            "221 0.40809181332588196\n",
            "222 0.38678544759750366\n",
            "223 0.36662018299102783\n",
            "224 0.34754711389541626\n",
            "225 0.3294195532798767\n",
            "226 0.31223946809768677\n",
            "227 0.295981764793396\n",
            "228 0.2806326150894165\n",
            "229 0.2660371959209442\n",
            "230 0.25217270851135254\n",
            "231 0.2390621304512024\n",
            "232 0.22666847705841064\n",
            "233 0.21491162478923798\n",
            "234 0.20373132824897766\n",
            "235 0.19311296939849854\n",
            "236 0.183098703622818\n",
            "237 0.1735861897468567\n",
            "238 0.16460835933685303\n",
            "239 0.15602776408195496\n",
            "240 0.14795853197574615\n",
            "241 0.14028319716453552\n",
            "242 0.13299372792243958\n",
            "243 0.126125767827034\n",
            "244 0.11957690119743347\n",
            "245 0.11337229609489441\n",
            "246 0.10752195119857788\n",
            "247 0.10195942223072052\n",
            "248 0.09667584300041199\n",
            "249 0.09167411178350449\n",
            "250 0.08689115196466446\n",
            "251 0.0824035257101059\n",
            "252 0.07816216349601746\n",
            "253 0.07412444055080414\n",
            "254 0.0702638253569603\n",
            "255 0.06664367765188217\n",
            "256 0.06318769603967667\n",
            "257 0.05995575338602066\n",
            "258 0.05686727911233902\n",
            "259 0.05392880737781525\n",
            "260 0.05113569647073746\n",
            "261 0.04850534349679947\n",
            "262 0.045976605266332626\n",
            "263 0.04362004995346069\n",
            "264 0.04136575385928154\n",
            "265 0.03924548253417015\n",
            "266 0.03721241652965546\n",
            "267 0.03529544547200203\n",
            "268 0.0334867388010025\n",
            "269 0.03178541362285614\n",
            "270 0.030141722410917282\n",
            "271 0.028580935671925545\n",
            "272 0.027128443121910095\n",
            "273 0.025724492967128754\n",
            "274 0.024407491087913513\n",
            "275 0.023152712732553482\n",
            "276 0.021970316767692566\n",
            "277 0.020837606862187386\n",
            "278 0.019777894020080566\n",
            "279 0.018770210444927216\n",
            "280 0.017813678830862045\n",
            "281 0.01689719781279564\n",
            "282 0.016039762645959854\n",
            "283 0.015216544270515442\n",
            "284 0.014445353299379349\n",
            "285 0.013710256665945053\n",
            "286 0.013001627288758755\n",
            "287 0.012338078580796719\n",
            "288 0.011729886755347252\n",
            "289 0.011127056553959846\n",
            "290 0.010568815283477306\n",
            "291 0.010045959614217281\n",
            "292 0.009535283781588078\n",
            "293 0.009063709527254105\n",
            "294 0.008605977520346642\n",
            "295 0.008175037801265717\n",
            "296 0.007768391631543636\n",
            "297 0.00738474540412426\n",
            "298 0.007017343305051327\n",
            "299 0.006673813331872225\n",
            "300 0.00633861031383276\n",
            "301 0.0060304636135697365\n",
            "302 0.0057284217327833176\n",
            "303 0.005450596567243338\n",
            "304 0.005185292102396488\n",
            "305 0.004931294359266758\n",
            "306 0.00468990346416831\n",
            "307 0.004465478006750345\n",
            "308 0.004245217889547348\n",
            "309 0.0040391553193330765\n",
            "310 0.0038442672230303288\n",
            "311 0.003663133131340146\n",
            "312 0.003494101809337735\n",
            "313 0.003328186459839344\n",
            "314 0.0031697149388492107\n",
            "315 0.003024607663974166\n",
            "316 0.002880377694964409\n",
            "317 0.002748603466898203\n",
            "318 0.0026261364109814167\n",
            "319 0.0025037832092493773\n",
            "320 0.0023872333113104105\n",
            "321 0.002278536092489958\n",
            "322 0.002175879431888461\n",
            "323 0.002081523882225156\n",
            "324 0.001991265919059515\n",
            "325 0.001902691088616848\n",
            "326 0.0018200852209702134\n",
            "327 0.0017371145077049732\n",
            "328 0.0016654353821650147\n",
            "329 0.0015929571818560362\n",
            "330 0.0015208314871415496\n",
            "331 0.0014585480093955994\n",
            "332 0.0013973067980259657\n",
            "333 0.0013392127584666014\n",
            "334 0.0012859436683356762\n",
            "335 0.0012309703743085265\n",
            "336 0.0011791227152571082\n",
            "337 0.001135081285610795\n",
            "338 0.0010913253063336015\n",
            "339 0.001047530327923596\n",
            "340 0.0010094300378113985\n",
            "341 0.0009701497619971633\n",
            "342 0.0009326641447842121\n",
            "343 0.0008964895969256759\n",
            "344 0.0008631731034256518\n",
            "345 0.0008300253539346159\n",
            "346 0.0007995602209120989\n",
            "347 0.0007695285021327436\n",
            "348 0.0007428210810758173\n",
            "349 0.0007168746669776738\n",
            "350 0.0006888429052196443\n",
            "351 0.0006665705586783588\n",
            "352 0.0006421093130484223\n",
            "353 0.0006200275965966284\n",
            "354 0.0005985437310300767\n",
            "355 0.0005773323937319219\n",
            "356 0.0005572647787630558\n",
            "357 0.0005382559029385448\n",
            "358 0.0005198352737352252\n",
            "359 0.0005027395091019571\n",
            "360 0.0004862982896156609\n",
            "361 0.0004714323440566659\n",
            "362 0.00045624293852597475\n",
            "363 0.00044215089292265475\n",
            "364 0.0004293563833925873\n",
            "365 0.0004161584365647286\n",
            "366 0.00040301866829395294\n",
            "367 0.0003897868446074426\n",
            "368 0.0003780036640819162\n",
            "369 0.0003658807254396379\n",
            "370 0.0003557837044354528\n",
            "371 0.0003444572212174535\n",
            "372 0.0003346162266097963\n",
            "373 0.00032580693368799984\n",
            "374 0.0003159063053317368\n",
            "375 0.0003068785008508712\n",
            "376 0.0002981138532049954\n",
            "377 0.00028907222440466285\n",
            "378 0.00028067745734006166\n",
            "379 0.0002735290036071092\n",
            "380 0.0002665440260898322\n",
            "381 0.0002596221165731549\n",
            "382 0.00025305955205112696\n",
            "383 0.00024595108698122203\n",
            "384 0.00023941425024531782\n",
            "385 0.0002338807680644095\n",
            "386 0.00022763175365980715\n",
            "387 0.00022153282770887017\n",
            "388 0.00021605382789857686\n",
            "389 0.0002102307480527088\n",
            "390 0.00020471739117056131\n",
            "391 0.00019992874877061695\n",
            "392 0.0001949167053680867\n",
            "393 0.00018963923503179103\n",
            "394 0.00018539369921199977\n",
            "395 0.00018029969942290336\n",
            "396 0.00017610634677112103\n",
            "397 0.00017234445840585977\n",
            "398 0.00016803578182589263\n",
            "399 0.0001641058479435742\n",
            "400 0.0001603512791916728\n",
            "401 0.0001565331913297996\n",
            "402 0.00015284193796105683\n",
            "403 0.00014968463801778853\n",
            "404 0.0001464030792703852\n",
            "405 0.00014342617942020297\n",
            "406 0.00013967214908916503\n",
            "407 0.00013685216254089028\n",
            "408 0.00013387536455411464\n",
            "409 0.00013136121560819447\n",
            "410 0.00012825839803554118\n",
            "411 0.0001262653386220336\n",
            "412 0.0001231721107615158\n",
            "413 0.00012070792581653222\n",
            "414 0.00011815479956567287\n",
            "415 0.00011572401854209602\n",
            "416 0.00011350557906553149\n",
            "417 0.00011110166815342382\n",
            "418 0.00010873096471186727\n",
            "419 0.00010638843377819285\n",
            "420 0.00010407276568002999\n",
            "421 0.00010214581561740488\n",
            "422 0.00010023840877693146\n",
            "423 9.863542072707787e-05\n",
            "424 9.668965503806248e-05\n",
            "425 9.494540427112952e-05\n",
            "426 9.273402974940836e-05\n",
            "427 9.188102558255196e-05\n",
            "428 9.006610343931243e-05\n",
            "429 8.805857942206785e-05\n",
            "430 8.63815366756171e-05\n",
            "431 8.485591388307512e-05\n",
            "432 8.375322067877278e-05\n",
            "433 8.221875759772956e-05\n",
            "434 8.067097223829478e-05\n",
            "435 7.919440395198762e-05\n",
            "436 7.799150625942275e-05\n",
            "437 7.630628533661366e-05\n",
            "438 7.496302714571357e-05\n",
            "439 7.387746154563501e-05\n",
            "440 7.268396439030766e-05\n",
            "441 7.120655936887488e-05\n",
            "442 7.008633110672235e-05\n",
            "443 6.895647675264627e-05\n",
            "444 6.807664613006637e-05\n",
            "445 6.689819565508515e-05\n",
            "446 6.56966440146789e-05\n",
            "447 6.440073775593191e-05\n",
            "448 6.388249312294647e-05\n",
            "449 6.289777957135811e-05\n",
            "450 6.16923498455435e-05\n",
            "451 6.103759369580075e-05\n",
            "452 5.997943662805483e-05\n",
            "453 5.9254354709992185e-05\n",
            "454 5.841602978762239e-05\n",
            "455 5.732458521379158e-05\n",
            "456 5.662050534738228e-05\n",
            "457 5.568716733250767e-05\n",
            "458 5.475685975397937e-05\n",
            "459 5.416479689301923e-05\n",
            "460 5.342999429558404e-05\n",
            "461 5.2613635489251465e-05\n",
            "462 5.20381800015457e-05\n",
            "463 5.1392791647231206e-05\n",
            "464 5.048024104326032e-05\n",
            "465 4.9616362957749516e-05\n",
            "466 4.87640718347393e-05\n",
            "467 4.803537740372121e-05\n",
            "468 4.7310804802691564e-05\n",
            "469 4.677681135945022e-05\n",
            "470 4.65020930278115e-05\n",
            "471 4.5843873522244394e-05\n",
            "472 4.5282205974217504e-05\n",
            "473 4.473958688322455e-05\n",
            "474 4.400997931952588e-05\n",
            "475 4.351001189206727e-05\n",
            "476 4.2933424992952496e-05\n",
            "477 4.216108573018573e-05\n",
            "478 4.15804679505527e-05\n",
            "479 4.100770820514299e-05\n",
            "480 4.037432881887071e-05\n",
            "481 3.993475183960982e-05\n",
            "482 3.976267180405557e-05\n",
            "483 3.907556310878135e-05\n",
            "484 3.8555546780116856e-05\n",
            "485 3.7904566852375865e-05\n",
            "486 3.746840957319364e-05\n",
            "487 3.727211515069939e-05\n",
            "488 3.671570084406994e-05\n",
            "489 3.6281897337175906e-05\n",
            "490 3.6026540328748524e-05\n",
            "491 3.554582144715823e-05\n",
            "492 3.5137436498189345e-05\n",
            "493 3.468761497060768e-05\n",
            "494 3.426578768994659e-05\n",
            "495 3.404750896152109e-05\n",
            "496 3.371261482243426e-05\n",
            "497 3.337107045808807e-05\n",
            "498 3.2921176170930266e-05\n",
            "499 3.2512394682271406e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycwxAPHZLNST",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Question 12\n",
        "\n",
        "In the code above, why do we have 2 in '2.0*(y_pred - y)`?\n",
        "\n",
        "Answer: differentaiton of x square is 2x, as loss function has the power of 2\n",
        "\n",
        "## Question 13\n",
        "In the code above, what does `grad_h[h < 0] = 0` signify?\n",
        "\n",
        "\n",
        "As relu switches  only for positive values, making negative values as zeros \n",
        "## Question 14\n",
        "In the code above, how many \"epochs\" have we trained the model for? \n",
        "\n",
        "\n",
        "1 epoch 500 iterations\n",
        "## Question 15\n",
        "In the code above, if we take the trained model, and run it on fresh  inputs, the trained model will be able to predict fresh output with high accuracy. \n",
        "\n",
        "**No the loss function convergesd to minima and increased back.**\n",
        "## Question 16\n",
        "In the code above, if we dont use clone in `grad_h = grad_h_relu.clone()` the model will still train without any issues. \n",
        "\n",
        "It will train without any issues, but since we dont have any clone there cant be any look back of values during back propagation it will overwrite according to my understanding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnJ49I07JJI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}